# Data Flow

This document outlines the paths data takes through the system during model training, inference, and monitoring.

## 1. Training Data Flow

The training pipeline is an offline process that takes raw data and produces a production-ready model registered in MLflow.

1.  **Data Ingestion:** Raw data (e.g., CSV or Parquet files) is loaded from a storage source.
2.  **Data Validation:** The raw data is validated against a schema to check for quality issues like missing values, incorrect data types, and unexpected values.
3.  **Feature Engineering:** The validated data is passed through a preprocessing and feature engineering pipeline to create model-ready features.
4.  **Data Splitting:** The dataset is split into training, validation, and test sets. This split can be stratified to maintain class distribution or time-based for a more realistic production simulation.
5.  **Model Training:** The model (e.g., XGBoost) is trained on the training set. Early stopping is used with the validation set to prevent overfitting.
6.  **MLflow Logging:** During the training run, all relevant information is logged to MLflow, including hyperparameters, performance metrics, and model artifacts.
7.  **Model Registration:** The trained model is registered in the MLflow Model Registry, creating a new version.
8.  **Validation & Promotion:** The registered model undergoes further validation tests. If it passes all checks, it is promoted to the "Production" stage.

## 2. Inference Data Flow

The inference flow is a real-time process that generates a prediction for a given input.

1.  **Client Request:** A client sends a request with applicant data (in JSON format) to the prediction API endpoint.
2.  **API Gateway:** The request is first handled by the API Gateway, which performs authentication and rate limiting.
3.  **Input Validation:** The FastAPI service validates the incoming data against a Pydantic schema to ensure it is well-formed.
4.  **Feature Engineering:** The service executes the feature engineering pipeline on the input data.
5.  **Feature Store Lookup (Optional):** The service may first query the Redis-based feature store to retrieve pre-computed features, falling back to on-the-fly computation if they are not in the cache.
6.  **Model Loading:** The prediction service loads the required model versions (Champion and Challenger) from the MLflow Model Registry. Models are cached in memory after the first load.
7.  **Champion-Challenger Routing:** The router decides whether to use the Champion or Challenger model for this specific request based on the configured traffic-splitting ratio.
8.  **Inference:** The selected XGBoost model's `predict` method is called to generate a probability score.
9.  **Post-processing:** The raw score is converted into a risk category (e.g., Low, Medium, High) and a final recommendation.
10. **Explanation Generation:** SHAP is used to generate an explanation for the prediction, identifying the key features that influenced the score.
11. **Response Formation:** The service constructs the final JSON response, including the prediction, explanation, and a unique request ID.
12. **Logging:** The request, features, model version, and final prediction are logged for monitoring and auditing purposes.
13. **Client Response:** The response is sent back to the client.
14. **Asynchronous Monitoring:** In the background, the prediction log is published to a queue or stream that feeds the monitoring service.

## 3. Monitoring Data Flow

The monitoring flow is a near-real-time process that analyzes prediction logs to detect system or model issues.

1.  **Log Aggregation:** Prediction logs generated by the inference service are collected and aggregated into hourly or daily batches.
2.  **Evidently AI Analysis:** The aggregated logs are fed into Evidently AI, which compares the recent data distribution against a reference dataset (e.g., the training data).
3.  **Drift Report Generation:** Evidently AI generates detailed reports on data drift, prediction drift, and (if ground truth labels are available) model performance degradation.
4.  **Metrics Export:** Key drift metrics (e.g., drift score, number of drifted features) are exported to Prometheus.
5.  **Dashboard Visualization:** A Grafana dashboard visualizes the drift metrics from Prometheus alongside infrastructure metrics (like latency and error rates), providing a unified view of system health.
6.  **Alerting:** If a metric crosses a predefined threshold (e.g., data drift is detected), an alert is automatically sent via Email, Slack, or another notification channel.
