{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - LendingClub Dataset\n",
    "\n",
    "## Project: Alternative Credit Scoring Platform\n",
    "\n",
    "This notebook performs initial exploratory data analysis on the LendingClub dataset to understand the features, distributions, and data quality issues. The dataset contains information about loans issued between 2007 and 2018, with a target variable indicating whether a loan defaulted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Set up plotting styles\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Let's load the LendingClub dataset and examine its basic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import os\n",
    "df = pd.read_csv('../data/raw/LC_loans_granting_model_dataset.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print('Dataset shape:', df.shape)\n",
    "print('\\nDataset info:')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "Based on the information from Zenodo, this dataset has been specifically prepared for granting models that make decisions on loan approval based on information available at application time. It includes loans with final status (either \"Fully Paid\" or \"Default\") to avoid data leakage.\n",
    "\n",
    "**Target Variable**: Default (binary)\n",
    "- 0 = Fully Paid\n",
    "- 1 = Default (charged off)\n",
    "\n",
    "### Features\n",
    "\n",
    "1. **Loan Identification**: `id`, `issue_d`\n",
    "2. **Quantitative Variables**: `revenue`, `dti_n`, `loan_amnt`, `fico_n`, `experience_c`\n",
    "3. **Categorical Variables**: `emp_length`, `purpose`, `home_ownership_n`, `addr_state`, `zip_code`\n",
    "4. **Textual Variables**: `title`, `desc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Analysis\n",
    "\n",
    "Let's examine the distribution of the target variable to understand the class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the target variable distribution\n",
    "target_counts = df['Default'].value_counts()\n",
    "target_percentages = df['Default'].value_counts(normalize=True) * 100\n",
    "\n",
    "print('Target variable distribution:')\n",
    "print(target_counts)\n",
    "print('\\nTarget variable percentages:')\n",
    "print(target_percentages)\n",
    "\n",
    "# Visualize the target distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(target_counts.values, labels=['Fully Paid (0)', 'Default (1)'], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Distribution of Target Variable')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=df, x='Default')\n",
    "plt.title('Count of Each Target Class')\n",
    "plt.xlabel('Default Status (0=Fully Paid, 1=Default)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values Analysis\n",
    "\n",
    "Let's check for missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "}).sort_values(by='Missing Percentage', ascending=False)\n",
    "\n",
    "print('Features with missing values (top 10):')\n",
    "print(missing_df[missing_df['Missing Count'] > 0].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Features Analysis\n",
    "\n",
    "Let's examine the distributions of numerical features and their relationship with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols.remove('Default')  # Remove target variable from numerical columns\n",
    "\n",
    "print('Numerical features:', numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for numerical features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    if i >= len(axes):\n",
    "        break\n",
    "        \n",
    "    # Plot distribution by target\n",
    "    sns.histplot(data=df, x=col, hue='Default', kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col} by Default Status')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Count')\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(len(numerical_cols), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical features by target\n",
    "numerical_stats = df.groupby('Default')[numerical_cols].describe()\n",
    "print('Statistical summary by Default status:')\n",
    "numerical_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features Analysis\n",
    "\n",
    "Let's examine the categorical features and their relationship with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print('Categorical features:', categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "for col in categorical_cols:\n",
    "    print(f'\\nFeature: {col}')\n",
    "    print(f'Unique values: {df[col].nunique()}')\n",
    "    print('Top 10 values:')\n",
    "    print(df[col].value_counts().head(10))\n",
    "    \n",
    "    # Create a cross-tabulation with the target variable\n",
    "    crosstab = pd.crosstab(df[col], df['Default'], normalize='index') * 100\n",
    "    print('\\nPercentage of defaults by category (top 10):')\n",
    "    print(crosstab.sort_values(by=1, ascending=False).head(10))\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical features vs target\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(categorical_cols[:4]):  # Show top 4 categorical features\n",
    "    # Calculate default rate by category\n",
    "    default_rate = df.groupby(col)['Default'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    # Create bar plot\n",
    "    sns.barplot(x=default_rate.index[:10], y=default_rate.values[:10], ax=axes[i])\n",
    "    axes[i].set_title(f'Default Rate by {col} (Top 10 Categories)')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Default Rate')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlations\n",
    "\n",
    "Let's examine correlations between numerical features to identify potential multicollinearity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Issues\n",
    "\n",
    "Let's check for potential data quality issues such as outliers, inconsistent values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in numerical features using IQR method\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Check outliers for each numerical feature\n",
    "for col in numerical_cols:\n",
    "    outliers = detect_outliers_iqr(df, col)\n",
    "    print(f'{col}: {len(outliers)} outliers ({len(outliers)/len(df)*100:.2f}% of data)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings and Hypotheses\n",
    "\n",
    "### Initial Findings:\n",
    "1. **Dataset Size**: The dataset contains 1,347,681 records with 15 features\n",
    "2. **Target Distribution**: The dataset is imbalanced with 80.02% of loans being fully paid and 19.98% defaulting\n",
    "3. **Missing Values**: The 'desc' field has 91.16% missing values, 'title' has 1.24% missing values, and 'zip_code' has 0.000074% missing values\n",
    "4. **Feature Relationships**: \n",
    "   - FICO scores appear to be lower for defaulted loans\n",
    "   - Debt-to-income ratios may be higher for defaulted loans\n",
    "   - Loan amounts seem to have similar distributions for both classes\n",
    "\n",
    "### Hypotheses:\n",
    "1. **FICO Score**: Lower FICO scores are likely associated with higher default rates\n",
    "2. **Debt-to-Income Ratio**: Higher DTI ratios may correlate with increased default risk\n",
    "3. **Loan Amount**: The relationship between loan amount and default risk may be complex\n",
    "4. **Employment Length**: Shorter employment history might indicate higher risk\n",
    "5. **Purpose**: Loan purpose may affect default rates (e.g. debt consolidation vs. small business)\n",
    "\n",
    "### Data Quality Notes:\n",
    "- The 'desc' field has a very high percentage of missing values and might need to be dropped or imputed\n",
    "- The 'id' field is likely just an identifier and may not be predictive\n",
    "- The 'revenue' field has some extreme outliers that might need to be handled\n",
    "- The 'fico_n' and 'dti_n' features appear to be normalized as suggested by their names ('_n' suffix)\n",
    "\n",
    "### Next Steps:\n",
    "1. Perform feature engineering to create additional predictive features\n",
    "2. Address missing values and outliers\n",
    "3. Prepare data for modeling\n",
    "4. Develop baseline models for comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}